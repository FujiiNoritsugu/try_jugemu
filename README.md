# try-jugemu

偶発的なコードを生成し、遺伝的アルゴリズムで進化させるPythonプログラム

## 概要

このプロジェクトは、ランダムなPythonコードを生成し、遺伝的アルゴリズムを用いてより優れたコードへと進化させるプログラムです。生成されたコードは自動的に実行され、エラーが発生した場合は自動修正を試みます。

## 機能

### 1. コード生成機能

- **ランダム識別子生成**: 変数名や関数名をランダムに生成
- **ランダム値生成**: int, str, bool, listの値をランダムに生成
- **ランダム関数生成**: パラメータ、処理内容、戻り値を持つ関数を生成
- **ランダムクラス生成**: 複数のメソッドを持つクラスを生成

### 2. 自動実行・修正機能

生成されたコードを実行し、エラーが発生した場合は自動的に修正を試みます（最大3回）。

**対応エラー:**
- **ゼロ除算エラー**: `// 0` や `% 0` を `// 1` や `% 1` に置き換え
- **構文エラー**: 空のブロックに適切なインデントで`pass`を追加
- **その他のエラー**: 新しいコードを生成して再試行

### 3. 遺伝的アルゴリズム

コードを個体として扱い、世代を重ねることでより優れたコードに進化させます。

#### 3.1 個体と適応度評価

**Individual クラス**:
- `code`: プログラムコード
- `fitness`: 適応度スコア

**適応度評価基準**:
- 実行成功: 100点
- 実行時間が短い: 最大20点
- コードの複雑さ（関数・クラスの数）: 関数5点/個、クラス10点/個
- 適度なコード長（20〜50行）: 10点
- エラーの種類による部分点:
  - ゼロ除算: 30点（修正可能）
  - その他のエラー: 20点
  - 構文エラー: 10点

#### 3.2 遺伝的操作

**選択（Selection）**:
- トーナメント選択方式
- 3個体からランダムに選び、最も適応度の高い個体を親として選択

**交叉（Crossover）**:
- 2つの親から関数とクラスを抽出
- ランダムに組み合わせて子を生成
- 関数: 最大3個、クラス: 最大2個を選択

**突然変異（Mutation）**:
- 確率: 20%
- 変異タイプ:
  - 新しい関数を追加
  - 新しいクラスを追加
  - 既存の関数を置き換え

**エリート保存**:
- 各世代の上位2個体を次世代に無条件で残す

#### 3.3 進化プロセス

1. 初期個体群を生成（デフォルト10個体）
2. 各個体の適応度を評価
3. 適応度でソートし、統計情報を表示
4. 次世代を生成:
   - 上位2個体をエリート保存
   - 残りはトーナメント選択→交叉→突然変異で生成
5. 指定世代数（デフォルト5世代）繰り返し
6. 最良個体のコードを実行

## 使い方

### インストール

```bash
cd try-jugemu
```

### 実行

```bash
python main.py
```

実行すると、モード選択が表示されます:

```
モードを選択してください
1: 通常
2: 遺伝的アルゴリズム
3: LLM改善
4: 遺伝的アルゴリズム+LLM
5: シミュレーテッドアニーリング
6: シミュレーテッドアニーリング+LLM
7: Q学習
8: Q学習+LLM
9: ハイブリッド (GA+SA+Q学習)
10: ハイブリッド+LLM
11: 保存されたコードを実行
選択 (1-11):
```

#### モード1: 通常モード

偶発的なコードを1つ生成し、実行します。エラーが発生した場合は自動修正を試みます。

```bash
1
```

#### モード2: 遺伝的アルゴリズムモード

10個体×5世代で進化させ、最良個体を実行します。

```bash
2
```

#### モード3: LLM改善モード

偶発的なコードを1つ生成し、LLMで意味のあるコードに改善してから実行します。

```bash
3
```

#### モード4: 遺伝的アルゴリズム+LLMモード

遺伝的アルゴリズムで進化させた後、LLMで改善して実行します。

```bash
4
```

#### モード5: シミュレーテッドアニーリングモード

温度パラメータを使った最適化手法で、徐々に温度を下げながらコードを最適化します。

```bash
5
```

#### モード6: シミュレーテッドアニーリング+LLMモード

シミュレーテッドアニーリングで最適化した後、LLMで改善して実行します。

```bash
6
```

#### モード7: Q学習モード

強化学習の一種であるQ学習を使って、コードを最適化します。経験から学習し、効果的な行動を選択します。

```bash
7
```

#### モード8: Q学習+LLMモード

Q学習で最適化した後、LLMで改善して実行します。

```bash
8
```

#### モード9: ハイブリッドモード（GA+SA+Q学習）

3つの最適化手法を段階的に適用し、各手法の長所を組み合わせます。

```bash
9
```

#### モード10: ハイブリッド+LLMモード

ハイブリッド最適化の後、LLMで改善して実行します。

```bash
10
```

#### モード11: 保存されたコードを実行

過去に生成・保存されたコードをロードして再実行します。

```bash
11
```

**出力例**:
```
============================================================
遺伝的アルゴリズムを開始します
個体数: 10, 世代数: 5
============================================================

【第1世代】
最高適応度: 145.00
平均適応度: 123.50
最良個体のコード（最初の5行）:
# 偶発的に生成されたコード

def x9a2b(p3c, q5d):
    """偶発的に生成された関数"""
    print(42)
...
```

### 4. シミュレーテッドアニーリング

温度パラメータを用いた最適化手法で、コードを徐々に改善します。

#### 4.1 アルゴリズムの概要

シミュレーテッドアニーリング（焼きなまし法）は、物理学の焼きなまし過程にヒントを得た最適化アルゴリズムです。高温時は悪い解も受け入れることで局所最適解から脱出し、温度を下げながら徐々に良い解に収束させます。

#### 4.2 最適化プロセス

1. **初期化**
   - 初期解（ランダムコード）を生成
   - 初期温度を設定（デフォルト: 100.0）

2. **反復最適化**
   - 現在の解を突然変異させて新しい解を生成
   - 新しい解の適応度を評価
   - メトロポリス基準で受理判定:
     - 適応度が向上: 必ず受理
     - 適応度が低下: 確率 `exp(Δ/T)` で受理（Δ: 適応度差、T: 温度）
   - 最良解を更新
   - 温度を下げる（デフォルト冷却率: 0.95）

3. **終了条件**
   - 温度が最低温度（デフォルト: 0.1）に達したら終了

#### 4.3 パラメータ

- **初期温度**: 100.0（高いほど初期段階で悪い解も受け入れやすい）
- **冷却率**: 0.95（各反復で温度に乗じる値、小さいほど急速に冷却）
- **最低温度**: 0.1（これ以下になったら終了）
- **突然変異率**: 0.3（遺伝的アルゴリズムより高めに設定）

#### 4.4 メトロポリス基準

適応度が `Δ` だけ悪化する場合、以下の確率で受理:

```
P(accept) = exp(Δ / T)
```

- 温度 `T` が高い: 悪化も受け入れやすい（探索的）
- 温度 `T` が低い: 良い解のみ受け入れる（収束的）

#### 4.5 遺伝的アルゴリズムとの比較

| 項目 | 遺伝的アルゴリズム | シミュレーテッドアニーリング |
|------|-------------------|----------------------------|
| 個体数 | 複数（10個体） | 単一 |
| 探索方法 | 交叉・突然変異 | 突然変異のみ |
| 局所最適解からの脱出 | 多様性で対応 | 確率的受理で対応 |
| 計算コスト | 高い | 低い |
| 収束速度 | 遅い | 速い |

**出力例**:
```
============================================================
シミュレーテッドアニーリングを開始します
初期温度: 100.0, 冷却率: 0.95, 最低温度: 0.1
============================================================

初期解の適応度: 125.00
初期コード（最初の5行）:
# 偶発的に生成されたコード

def x9a2b(p3c, q5d):
    """偶発的に生成された関数"""

[反復 1] 🌟 最良解更新! 適応度: 135.00, 温度: 100.00
[反復 10] 温度: 59.87, 現在: 140.00, 最良: 145.00, 状態: 改善
[反復 20] 温度: 35.85, 現在: 135.00, 最良: 145.00, 状態: 確率的受理 (p=0.1889)
...

============================================================
最適化完了！総反復回数: 67
最良解の適応度: 165.00
============================================================
```

### 5. Q学習（強化学習）

経験から学習し、最適な行動を選択する強化学習の一種であるQ学習を使ってコードを最適化します。

#### 5.1 アルゴリズムの概要

Q学習は、**状態（State）**と**行動（Action）**に対する**価値（Q値）**を学習し、報酬を最大化する行動を選択する強化学習アルゴリズムです。経験を通じて効果的な行動パターンを学習できるため、ランダムな探索よりも効率的です。

#### 5.2 状態空間（State Space）

コードの特徴を以下の4つのカテゴリに離散化:

- **関数数カテゴリ**: 0-4（0個、2-3個、4-5個、...）
- **クラス数カテゴリ**: 0-3（0個、1個、2個、3個以上）
- **コード行数カテゴリ**: 0-5（0-9行、10-19行、20-29行、...）
- **適応度カテゴリ**: 0-5（0-49点、50-99点、100-149点、...）

例: 状態 `(2, 1, 3, 2)` = 関数4-5個、クラス1個、コード20-29行、適応度100-149点

#### 5.3 行動空間（Action Space）

7種類の行動を定義:

1. **add_function**: 関数を追加
2. **remove_function**: 関数を削除
3. **add_class**: クラスを追加
4. **remove_class**: クラスを削除
5. **modify_operator**: 演算子を変更（+, -, *, //, %）
6. **mutate**: 既存の突然変異を適用
7. **no_action**: 何もしない

**重要な改善点:**
行動適用後に構文チェックを実施し、エラーが検出された場合は元の個体を保持します。これにより、構文エラーのあるコードが最良個体として選ばれることを防ぎます。

#### 5.4 報酬関数（Reward Function）

適応度の差分を報酬として使用:

```
報酬 = 新しい適応度 - 現在の適応度
```

- 適応度が向上: 正の報酬
- 適応度が低下: 負の報酬
- 変化なし: 報酬0

#### 5.5 Q学習の更新式

Q値は以下の式で更新されます:

```
Q(s,a) ← Q(s,a) + α[r + γ・max Q(s',a') - Q(s,a)]
```

- `Q(s,a)`: 状態sで行動aを取ったときの価値
- `α`: 学習率（デフォルト: 0.1）
- `r`: 報酬
- `γ`: 割引率（デフォルト: 0.9）
- `s'`: 次の状態
- `max Q(s',a')`: 次状態での最大Q値

#### 5.6 ε-greedy方策

探索と活用のバランスを取るため、ε-greedy方策を使用:

- **εの確率でランダム探索**（未知の行動を試す）
- **1-εの確率で最良行動を選択**（学習済みの知識を活用）
- εは学習が進むにつれて減衰（1.0 → 0.1）

#### 5.7 学習プロセス

1. **初期化**
   - Q-table（状態-行動ペアのQ値辞書）を作成
   - 探索率ε = 1.0（最初は全てランダム探索）

2. **エピソードループ**（デフォルト: 50エピソード）
   - 新しいランダムコードを生成（初期状態）
   - εを徐々に減衰（1.0 → 0.1）

3. **ステップループ**（各エピソード最大20ステップ）
   - ε-greedyで行動を選択
   - 行動を適用して新しいコードを生成
   - 適応度を評価し報酬を計算
   - Q値を更新
   - 次の状態へ遷移

4. **最良個体の記録**
   - 全エピソードを通じて最も高い適応度のコードを保存

#### 5.8 他の手法との比較

| 項目 | 遺伝的アルゴリズム | シミュレーテッドアニーリング | **Q学習** |
|------|-------------------|----------------------------|-----------|
| 探索方法 | 交叉・突然変異 | 確率的受理 | **経験に基づく行動選択** |
| 学習 | なし | なし | **あり（Q値学習）** |
| 個体数 | 複数（10個体） | 単一 | **単一（各エピソード）** |
| 知識の蓄積 | なし | なし | **あり（Q-table）** |
| 適応性 | 低い | 中程度 | **高い（パターン学習）** |
| 計算コスト | 高い | 低い | **中程度** |

#### 5.9 パラメータ調整

```python
q_learning(
    episodes=50,        # エピソード数（学習回数）
    max_steps=20,       # 各エピソードの最大ステップ数
    use_llm=False       # LLMによる改善の有無
)

# Q-tableのパラメータ
QTable(
    learning_rate=0.1,      # 学習率α（0.0-1.0、大きいほど新しい経験を重視）
    discount_factor=0.9     # 割引率γ（0.0-1.0、大きいほど将来の報酬を重視）
)

# ε-greedyのパラメータ
epsilon_start=1.0      # 初期探索率（100%ランダム）
epsilon_end=0.1        # 最終探索率（10%ランダム、90%最良選択）
```

**出力例**:
```
============================================================
Q学習を開始します
エピソード数: 50, 最大ステップ数: 20
============================================================

【エピソード 1/50】探索率ε: 1.000
  [ステップ 3] 🌟 最良個体更新! 行動: add_function, 適応度: 135.00
  [ステップ 7] 🌟 最良個体更新! 行動: mutate, 適応度: 150.00
  エピソード報酬: 45.00, 最高適応度: 150.00

【エピソード 10/50】探索率ε: 0.820
  学習済みQ値数: 234, 平均Q値: 12.35
  エピソード報酬: 60.00, 最高適応度: 155.00

【エピソード 20/50】探索率ε: 0.640
  学習済みQ値数: 456, 平均Q値: 18.72
  エピソード報酬: 70.00, 最高適応度: 165.00

【エピソード 50/50】探索率ε: 0.100
  学習済みQ値数: 892, 平均Q値: 25.48
  エピソード報酬: 50.00, 最高適応度: 170.00

============================================================
Q学習完了！
最良個体の適応度: 170.00
============================================================
```

### 6. ハイブリッド最適化（GA + SA + Q学習）

3つの最適化手法を段階的に組み合わせ、各手法の長所を活かした最も強力な最適化アルゴリズムです。

#### 6.1 アルゴリズムの概要

ハイブリッド最適化は、**遺伝的アルゴリズム（探索）** → **シミュレーテッドアニーリング（収束）** → **Q学習（学習ベース微調整）**の3段階で実行され、各手法の弱点を補完します。

#### 6.2 最適化フロー

```
初期状態（ランダムコード）
    ↓
【フェーズ1: 遺伝的アルゴリズム】
    目的: 大域的探索
    - 10個体 × 5世代
    - 交叉・突然変異で多様性を確保
    - 広い解空間を探索
    ↓
GA最良個体（適応度: 例 145.0）
    ↓
【フェーズ2: シミュレーテッドアニーリング】
    目的: 局所最適化
    - GAの最良個体から開始
    - 温度パラメータで探索と収束を制御
    - 細かい改善を実施
    ↓
SA最良個体（適応度: 例 165.0, +20.0改善）
    ↓
【フェーズ3: Q学習】
    目的: 学習ベース微調整
    - SAの最良個体から学習開始
    - 30エピソード × 15ステップ
    - 効果的な行動パターンを学習
    ↓
最終個体（適応度: 例 175.0, +10.0改善、総合+30.0）
```

#### 6.3 各フェーズの役割

**フェーズ1: 遺伝的アルゴリズム（大域的探索）**
- **目的**: 解空間全体を広く探索
- **特徴**:
  - 多様な個体群による並列探索
  - 交叉による構造の組み合わせ
  - 局所最適解に陥りにくい
- **出力**: 多様性のある高品質な初期解

**フェーズ2: シミュレーテッドアニーリング（局所最適化）**
- **目的**: GAで見つけた解の周辺を精密に探索
- **特徴**:
  - 単一個体による集中的な探索
  - 温度パラメータによる段階的収束
  - 確率的受理で局所最適解から脱出
- **出力**: 局所的に最適化された解

**フェーズ3: Q学習（学習ベース微調整）**
- **目的**: 経験から学習し、効果的な変更を適用
- **特徴**:
  - 過去の成功パターンを記憶
  - Q-tableによる行動価値の学習
  - 適応的な行動選択
- **出力**: 学習により最適化された最終解

#### 6.4 各手法の補完関係

| 手法 | 得意領域 | 苦手領域 | ハイブリッドでの役割 |
|------|---------|---------|-------------------|
| **GA** | 大域的探索 | 局所最適化 | **初期探索** - 良質な出発点を提供 |
| **SA** | 局所最適化 | 大域的探索 | **中間収束** - GAの解を洗練 |
| **Q学習** | パターン学習 | 初期探索 | **最終調整** - 効果的な変更を学習 |

#### 6.5 パラメータ設定

```python
hybrid_optimization(use_llm=False)

# フェーズ1: GA
population_size = 10    # 個体数
generations = 5         # 世代数

# フェーズ2: SA
initial_temp = 100.0    # 初期温度
cooling_rate = 0.95     # 冷却率
min_temp = 0.1          # 最低温度

# フェーズ3: Q学習
episodes = 30           # エピソード数（GAの後なので短め）
max_steps = 15          # 最大ステップ数
epsilon_start = 0.5     # 初期探索率（既に良い解があるので低め）
epsilon_end = 0.1       # 最終探索率
```

#### 6.6 他の手法との比較

| 項目 | GA単体 | SA単体 | Q学習単体 | **ハイブリッド** |
|------|--------|--------|-----------|-----------------|
| 探索範囲 | 広い | 狭い | 中程度 | **最も広い** |
| 収束速度 | 遅い | 速い | 中程度 | **バランス良好** |
| 最終品質 | 中程度 | 高い | 中〜高 | **最も高い** |
| 学習能力 | なし | なし | あり | **あり** |
| 計算コスト | 高い | 低い | 中程度 | **最も高い** |
| 適応度改善 | +20-30 | +15-25 | +10-20 | **+30-50** |

#### 6.7 実行時の出力例

```
============================================================
ハイブリッド最適化を開始します
手法: 遺伝的アルゴリズム → シミュレーテッドアニーリング → Q学習
============================================================

🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬
【フェーズ1: 遺伝的アルゴリズム】
目的: 多様なコード構造を生成し、大域的に探索
🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬 🧬

【GA 第1/5世代】
  最高適応度: 135.00, 平均適応度: 115.50
【GA 第2/5世代】
  最高適応度: 140.00, 平均適応度: 125.00
【GA 第5/5世代】
  最高適応度: 145.00, 平均適応度: 135.00

✅ GA完了: 最良適応度 = 145.00

🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥
【フェーズ2: シミュレーテッドアニーリング】
目的: GAの最良個体を起点に局所最適化
🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥 🔥

初期解の適応度: 145.00
  [SA 反復 2] 🌟 最良解更新! 適応度: 150.00
  [SA 反復 15] 🌟 最良解更新! 適応度: 165.00

✅ SA完了: 最良適応度 = 165.00 (改善: +20.00)

🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠
【フェーズ3: Q学習】
目的: 経験から学習し、効果的な行動で微調整
🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠 🧠

  [QL エピソード 3, ステップ 5] 🌟 最良個体更新! 適応度: 170.00
  [QL エピソード 12, ステップ 8] 🌟 最良個体更新! 適応度: 175.00
  [QL エピソード 10/30] 現在の最良適応度: 175.00
  [QL エピソード 20/30] 現在の最良適応度: 175.00
  [QL エピソード 30/30] 現在の最良適応度: 175.00

✅ Q学習完了: 最良適応度 = 175.00 (改善: +10.00)

============================================================
ハイブリッド最適化完了！
============================================================
GA最良適応度:     145.00
SA最良適応度:     165.00 (+20.00)
Q学習最良適応度:  175.00 (+10.00)
総合改善:         +30.00
============================================================
```

#### 6.8 利点と注意点

**利点:**
- ✅ **最高の最適化性能**: 3つの手法の長所を統合
- ✅ **バランスの良い探索**: 大域探索 → 局所最適化 → 学習ベース調整
- ✅ **ロバスト性**: 各フェーズが前フェーズの弱点を補完
- ✅ **段階的改善**: 各フェーズで確実に品質向上

**注意点:**
- ⚠️ **計算コスト**: 3つの手法を実行するため時間がかかる
- ⚠️ **パラメータ調整**: 各フェーズのバランスが重要
- ⚠️ **過学習リスク**: Q学習で特定パターンに偏る可能性

### 7. コード保存・ロード機能

生成されたコードを自動保存し、後で再利用できる機能です。

#### 7.1 機能概要

すべてのモード（1-10）で生成されたコードは、実行後に自動的に`generated_codes/`ディレクトリに保存されます。保存されたコードは、モード11から選択してロード・実行できます。

#### 7.2 保存されるファイル

**ファイル名形式:**
```
code_{モード名}_{タイムスタンプ}.py
```

**例:**
- `code_GA_20260212_143022.py` - 遺伝的アルゴリズムで生成
- `code_Hybrid+LLM_20260212_150130.py` - ハイブリッド+LLMで生成
- `code_Q-Learning_20260212_152045.py` - Q学習で生成

**ファイル内容:**
```python
# 生成コードメタデータ
# モード: Hybrid
# 生成日時: 2026-02-12 15:30:45
# 適応度: 175.00
# ==========================================================

# 偶発的に生成されたコード（遺伝的交叉）

def x9a2b(p3c, q5d):
    """偶発的に生成された関数"""
    print(42)
    return True
...
```

#### 7.3 使い方

**1. コードの自動保存**

すべてのモード（1-10）で、コード生成・最適化が完了すると自動的に保存されます。

```
============================================================
💾 生成されたコードを保存しました
============================================================
保存先: generated_codes/code_Hybrid_20260212_143022.py
============================================================
```

**2. 保存されたコードをロードして実行**

モード11を選択すると、保存されたコードの一覧が表示されます。

```bash
python main.py
# 11を選択

============================================================
💾 保存されたコード一覧
============================================================
1. code_Hybrid_20260212_152045.py
   モード: Hybrid
   生成日時: 2026-02-12 15:20:45
   適応度: 175.00

2. code_Q-Learning_20260212_143022.py
   モード: Q-Learning
   生成日時: 2026-02-12 14:30:22
   適応度: 165.00

3. code_GA+LLM_20260212_135510.py
   モード: GA+LLM
   生成日時: 2026-02-12 13:55:10
   適応度: 150.00

============================================================

ロードするコードの番号を入力してください (0: キャンセル): 1

============================================================
✅ コードをロードしました: code_Hybrid_20260212_152045.py
============================================================

ロードされたコードを実行します:
...
```

#### 7.4 利点

✅ **履歴管理**: 過去に生成した高品質なコードを保存
✅ **再現性**: 同じコードを何度でも実行可能
✅ **比較分析**: 異なるモードで生成されたコードを比較
✅ **メタデータ**: 生成モード、日時、適応度を記録
✅ **進化の追跡**: 各最適化手法の結果を時系列で確認

#### 7.5 保存ディレクトリ

生成されたコードは以下のディレクトリに保存されます:

```
try-jugemu/
├── main.py
├── README.md
└── generated_codes/        # 自動生成されるディレクトリ
    ├── code_GA_20260212_143022.py
    ├── code_SA_20260212_144530.py
    ├── code_Q-Learning_20260212_150100.py
    └── code_Hybrid+LLM_20260212_152045.py
```

**注意:** `generated_codes/`ディレクトリは初回実行時に自動作成されます。

## コード構成

### 主要関数

- `generate_random_identifier(length)`: ランダムな識別子を生成
- `generate_random_value()`: ランダムな値を生成
- `generate_random_operation()`: ランダムな操作を生成
- `generate_random_function()`: ランダムな関数を生成
- `generate_random_class()`: ランダムなクラスを生成
- `generate_code(num_functions, num_classes)`: 完全なコードを生成

### エラー修正関数

- `fix_division_by_zero(code)`: ゼロ除算を修正
- `fix_syntax_error(code)`: 構文エラーを修正
- `execute_generated_code(code, max_retries)`: コード実行とエラー修正

### 遺伝的アルゴリズム関数

- `Individual`: 個体クラス
  - `evaluate_fitness()`: 適応度評価
  - `extract_functions()`: 関数抽出
  - `extract_classes()`: クラス抽出
- `selection(population, tournament_size)`: トーナメント選択
- `crossover(parent1, parent2)`: 交叉操作
- `mutate(individual, mutation_rate)`: 突然変異
- `genetic_algorithm(population_size, generations)`: メインループ

### シミュレーテッドアニーリング関数

- `simulated_annealing(initial_temp, cooling_rate, min_temp, use_llm)`: シミュレーテッドアニーリングのメインループ
  - 初期解の生成と評価
  - 温度管理と冷却
  - メトロポリス基準による受理判定
  - 最良解の追跡

### Q学習（強化学習）関数

- `extract_state(individual)`: 個体から状態を抽出（離散化された特徴量）
  - 関数数、クラス数、コード行数、適応度をカテゴリ化
- `apply_action(individual, action)`: 行動を個体に適用
  - add_function, remove_function, add_class, remove_class, modify_operator, mutate, no_action
  - 行動適用後に構文チェックを実施し、エラーがあれば元の個体を返す
- `QTable`: Q-tableクラス
  - `get_q_value(state, action)`: Q値を取得
  - `update_q_value(state, action, reward, next_state)`: Q学習の更新式でQ値を更新
  - `get_best_action(state)`: 最良の行動を選択
  - `choose_action(state, epsilon)`: ε-greedy方策で行動を選択
- `q_learning(episodes, max_steps, use_llm)`: Q学習のメインループ
  - エピソードごとに学習
  - ε-greedy探索と活用
  - Q値の更新と最良個体の記録

### ハイブリッド最適化関数

- `hybrid_optimization(use_llm)`: ハイブリッド最適化のメインループ
  - フェーズ1: 遺伝的アルゴリズム（大域的探索）
  - フェーズ2: シミュレーテッドアニーリング（局所最適化）
  - フェーズ3: Q学習（学習ベース微調整）
  - 各フェーズの最良個体を次フェーズに引き継ぎ
  - 段階的改善サマリーを表示

### コード保存・ロード関数

- `save_generated_code(code, mode_name, fitness)`: 生成されたコードをファイルに保存
  - タイムスタンプ付きファイル名で保存
  - メタデータ（モード、日時、適応度）をコメントとして記録
  - `generated_codes/`ディレクトリに保存
- `list_saved_codes()`: 保存されたコードの一覧を表示
  - ファイルを更新日時の新しい順にソート
  - メタデータを抽出して表示
- `load_saved_code()`: 保存されたコードをロード
  - 一覧から番号で選択
  - メタデータ部分を除去してコードのみ返す

### LLM関連関数

- `improve_code_with_llm(code)`: LLMを使ってコードを改善
- `evaluate_code_with_llm(original_code, improved_code)`: LLMを使ってコードを評価

## カスタマイズ

### パラメータ調整

`main.py`の以下の値を変更することでカスタマイズできます:

```python
# 遺伝的アルゴリズムのパラメータ
genetic_algorithm(
    population_size=10,  # 個体数
    generations=5        # 世代数
)

# シミュレーテッドアニーリングのパラメータ
simulated_annealing(
    initial_temp=100.0,   # 初期温度（高いほど探索的）
    cooling_rate=0.95,    # 冷却率（小さいほど急速に収束）
    min_temp=0.1          # 最低温度（終了条件）
)

# 突然変異率
mutate(individual, mutation_rate=0.2)  # 20%（GAの場合）
mutate(individual, mutation_rate=0.3)  # 30%（SAの場合）

# トーナメントサイズ
selection(population, tournament_size=3)  # 3個体

# 初期コード生成
generate_code(
    num_functions=3,  # 関数の数
    num_classes=2     # クラスの数
)
```

## 技術仕様

- **言語**: Python 3.12+
- **依存ライブラリ**:
  - 標準ライブラリ: random, string, time, re, math, os
  - 外部ライブラリ: anthropic, python-dotenv（LLM機能使用時）
- **プロジェクト管理**: pyproject.toml

## ライセンス

このプロジェクトは実験的なコード生成プログラムです。生成されたコードの実行は自己責任で行ってください。

## 今後の拡張案

- より高度な適応度関数（出力の多様性、特定のパターンの検出など）
- 複数の親から交叉する多点交叉
- コードの意味的な解析による突然変異
- 進化の過程を可視化する機能（適応度の推移グラフなど）
- 生成されたコードのデータベース保存
- 特定の目標（例: 特定の出力を生成する）に向けた進化
- 他の最適化アルゴリズムの追加:
  - 粒子群最適化（PSO）
  - 差分進化（DE）
  - ベイズ最適化
  - ✅ ~~強化学習ベースのコード生成~~（Q学習として実装済み）
  - Deep Q-Network（DQN）
  - Actor-Critic
  - PPO（Proximal Policy Optimization）
- ✅ ~~ハイブリッド手法（遺伝的アルゴリズム + シミュレーテッドアニーリング + Q学習）~~（実装済み）
- ✅ ~~コード保存・ロード機能~~（実装済み）
- Q-tableの永続化（学習済みQ値の保存と再利用）
- ハイブリッド手法の並列実行版（各手法を独立に実行し最良を選択）
- コードのバージョン管理（同じモードでの改善履歴を追跡）
- 保存されたコードの検索・フィルタリング機能
- コード間の差分表示機能
